{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262ec198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2026-02-25 10:11:01.255062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "import tempfile\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d8effc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖChromaDB client initialized\n",
      "‚úÖCollection 'techcop_rag' ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test embedding created: 384 dimension\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "print(f\"‚úÖChromaDB client initialized\")\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"techcop_rag\")\n",
    "print(f\"‚úÖCollection '{collection.name}' ready\")\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "test_text = \"Testing RAG setup\"\n",
    "test_embedding = model.encode(test_text)\n",
    "print(f\"‚úÖ Test embedding created: {len(test_embedding)} dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ae09eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: products_datastream_chunk_0\n",
      "Add of existing embedding ID: products_datastream_chunk_0\n",
      "Insert of existing embedding ID: products_datastream_chunk_1\n",
      "Add of existing embedding ID: products_datastream_chunk_1\n",
      "Insert of existing embedding ID: products_cloudsync_chunk_0\n",
      "Add of existing embedding ID: products_cloudsync_chunk_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üóÇÔ∏è Processing products\n",
      " datastream.md: 2 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: products_cloudsync_chunk_1\n",
      "Add of existing embedding ID: products_cloudsync_chunk_1\n",
      "Insert of existing embedding ID: policies_remote_work_chunk_0\n",
      "Add of existing embedding ID: policies_remote_work_chunk_0\n",
      "Insert of existing embedding ID: policies_remote_work_chunk_1\n",
      "Add of existing embedding ID: policies_remote_work_chunk_1\n",
      "Insert of existing embedding ID: policies_dress_code_chunk_0\n",
      "Add of existing embedding ID: policies_dress_code_chunk_0\n",
      "Insert of existing embedding ID: policies_dress_code_chunk_1\n",
      "Add of existing embedding ID: policies_dress_code_chunk_1\n",
      "Insert of existing embedding ID: support_vpn_troubleshooting_chunk_0\n",
      "Add of existing embedding ID: support_vpn_troubleshooting_chunk_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cloudsync.md: 2 chunks\n",
      "\n",
      " üóÇÔ∏è Processing policies\n",
      " remote_work.md: 2 chunks\n",
      " dress_code.md: 2 chunks\n",
      "\n",
      " üóÇÔ∏è Processing support\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: support_vpn_troubleshooting_chunk_1\n",
      "Add of existing embedding ID: support_vpn_troubleshooting_chunk_1\n",
      "Insert of existing embedding ID: support_password_reset_chunk_0\n",
      "Add of existing embedding ID: support_password_reset_chunk_0\n",
      "Insert of existing embedding ID: support_password_reset_chunk_1\n",
      "Add of existing embedding ID: support_password_reset_chunk_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vpn_troubleshooting.md: 2 chunks\n",
      " password_reset.md: 2 chunks\n",
      "--------------------------------------------------\n",
      "    Document processed: 6\n",
      "    Total chunks created: 12\n",
      "    Collection size: 12\n"
     ]
    }
   ],
   "source": [
    "def smart_chunk(text, overlap_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Smart paragraph-based chunking with overlap\n",
    "    \"\"\"\n",
    "\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(len(paragraphs)):\n",
    "        chunk_parts = []\n",
    "        chunk_parts.append(paragraphs[i])\n",
    "\n",
    "        if i + 1 < len(paragraphs):\n",
    "            chunk_parts.append(paragraphs[i+1])\n",
    "\n",
    "        if i > 0 and overlap_ratio > 0:\n",
    "            overlap_chars = int(len(paragraphs[i-1]) * overlap_ratio)\n",
    "            chunk_parts.insert(0,paragraphs[i-1][-overlap_chars:])\n",
    "\n",
    "        chunk = \" \".join(chunk_parts)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "doc_dir = Path(\"techcorp-docs/techcorp-docs\")\n",
    "total_chunks = 0\n",
    "docs_processed = 0\n",
    "\n",
    "for category_dir in doc_dir.iterdir():\n",
    "    if category_dir.is_dir():\n",
    "        print(f\"\\n üóÇÔ∏è Processing {category_dir.name}\")\n",
    "\n",
    "        for doc_file in category_dir.glob(\"*.md\"):\n",
    "            metadata = {\n",
    "                \"source\": doc_file.name,\n",
    "                \"section\": category_dir.name\n",
    "            }\n",
    "\n",
    "            with open(doc_file, \"r\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            chunks = smart_chunk(content)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{category_dir.name}_{doc_file.stem}_chunk_{i}\"\n",
    "                embedding = model.encode(chunk).tolist()\n",
    "\n",
    "                collection.add(\n",
    "                    ids=[chunk_id],\n",
    "                    embeddings=[embedding],\n",
    "                    documents=[chunk],\n",
    "                    metadatas=[metadata]\n",
    "                )\n",
    "                total_chunks +=1\n",
    "            docs_processed +=1\n",
    "            print(f\" {doc_file.name}: {len(chunks)} chunks\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(f\"    Document processed: {docs_processed}\")\n",
    "print(f\"    Total chunks created: {total_chunks}\")\n",
    "print(f\"    Collection size: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7843b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    )\n",
    "def test_generation():\n",
    "    temperature = 0.3\n",
    "    max_tokens = 500\n",
    "\n",
    "    llm.temperature = temperature\n",
    "    llm.max_tokens = max_tokens\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role:\": \"user\", \"content\": \"What is RAG in AI? Answer in one sentence\"}\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    answer = response.content\n",
    "\n",
    "    print(f\"Test response: {answer}\")\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdecbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You can work from home up to 3 days per week.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 151, 'total_tokens': 164, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c5f3afa7ce', 'finish_reason': 'stop', 'logprobs': None} id='run-75c96c2e-8a0f-4f1a-8a01-ce09b3f479d4-0' usage_metadata={'input_tokens': 151, 'output_tokens': 13, 'total_tokens': 164, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "ü§ñ Generated answer:\n",
      "----------------------------------------\n",
      "You can work from home up to 3 days per week.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 4: Prompt Engineering\n",
    "Build the RAG prompt template that ensures accurate, context-based answers\n",
    "\"\"\"\n",
    "def create_rag_prompt(context_chunks, user_question):\n",
    "    \"\"\"Create the RAF prompt with context and question\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are TechCorp's helpful AI assistant.\n",
    "Answer ONLY based on the provided context.\n",
    "If the answer is not in the context, say 'I don't have that information in the provided documents'.\n",
    "Be concise and accurate\"\"\"\n",
    "\n",
    "    context_text = \"Context from TechCorp documents:\\n\\n\"\n",
    "    for i, chunk in enumerate(context_chunks,1):\n",
    "        context_text += f\"[Document {i}]\\n{chunk}\\n\\n\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "{context_text}\n",
    "\n",
    "Question: {user_question}\n",
    "Answer:\"\"\" \n",
    "    \n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "def test_prompt_engineering():\n",
    "    test_chunks = [\n",
    "        \"TechCorp allows employees to work remotely up to 3 days per week. Core hours are 10 AM to 3 PM.\",\n",
    "        \"Remote work arrangements must be approved by your manager and documented with HR.\",\n",
    "        \"VPN is mandatory when accessing company resources from home.\"\n",
    "    ]\n",
    "\n",
    "    test_question = \"How many days can I work from home?\"\n",
    "\n",
    "    system_prompt = \"\"\"You are TechCorp's helpful AI assistant.\n",
    "Answer ONLY based on the provided context.\n",
    "If the answer is not in the context, say 'I don't have that information in the provided documents'.\n",
    "Be concise and accuratte\"\"\"\n",
    "\n",
    "    context_text = \"Context from TechCorp documents:\\n\\n\"\n",
    "    for i, chunk in enumerate(test_chunks, 1):\n",
    "        context_text += f\"[Document {i}]\\n{chunk}\\n\\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Use the following context to answer the question.\"},\n",
    "        {\"role\": \"assistant\", \"content\": context_text},\n",
    "        {\"role\": \"user\", \"content\": test_question},\n",
    "    ]\n",
    "\n",
    "    response_1 = llm.invoke(messages)\n",
    "    print(response_1)\n",
    "    print(\"\\nü§ñ Generated answer:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(response_1.content)\n",
    "\n",
    "    return True\n",
    "\n",
    "success = test_prompt_engineering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eae990db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 5: Complete RAG pipeline\"\"\"\n",
    "# client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# collection = client.get_or_create_collection(name=\"techcop_rag\")\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "def rag_pipeline(user_question):\n",
    "    \"\"\"Complete RAG pipeline: Retrieve -> Augment -> Generate\"\"\"\n",
    "\n",
    "    print(f\"\\n‚ùî Question: {user_question}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    # STEP 1: RETRIEVE\n",
    "    print(f\"1Ô∏è‚É£ RETRIEVE: Converting to embeddings...\")\n",
    "    query_embeddings = model.encode(user_question).tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embeddings],\n",
    "        n_results=3\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "\n",
    "    print(f\"     Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "    for i, meta in enumerate(metadatas):\n",
    "        print(f\"    - {meta['source']} ({meta['section']})\")\n",
    "\n",
    "    # STEP 2: AUGMENT\n",
    "    print(\"\\n2Ô∏è‚É£ AUGMENT: Building context...\")\n",
    "\n",
    "    system_prompt = \"\"\"You are TechCorp's helpful AI assistant.\n",
    "Answer ONLY based on the provided context.\n",
    "If the answer is not in the context, say 'I don't have that information in the provided documents'.\"\"\" \n",
    "\n",
    "    context_text_1 = \"Context from TechCorp documents:\\n\\n\"\n",
    "    for i, chunk in enumerate(retrieved_chunks,1):\n",
    "        context_text_1 += f\"[Document {i}]\\n{chunk}\\n\\n\"\n",
    "\n",
    "    user_prompt_1=f\"{context_text_1}\\nQuestion: {user_question}\\n\\nAnswer:\"\n",
    "\n",
    "    print(\" ‚úÖ Context prepared with retrieved document\")\n",
    "\n",
    "    # STEP 3: GENERATE\n",
    "    print(\"n3Ô∏è‚É£ GENERATE: Creating answer...\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_1}\n",
    "    ]\n",
    "\n",
    "    response_1 = llm.invoke(messages)\n",
    "    answer_1 = response_1.content\n",
    "\n",
    "    sources = [meta['source'] for meta in metadatas]\n",
    "    unique_sources = list(set(sources))\n",
    "\n",
    "    final_response = f\"{answer_1}\\n\\nüìé Sources: {', '.join(unique_sources)}\"\n",
    "\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4145a50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùî Question: Can I bring my dog to the office?\n",
      "--------------------------------------------------\n",
      "1Ô∏è‚É£ RETRIEVE: Converting to embeddings...\n",
      "     Retrieved 3 relevant chunks\n",
      "    - dress_code.md (policies)\n",
      "    - dress_code.md (policies)\n",
      "    - remote_work.md (policies)\n",
      "\n",
      "2Ô∏è‚É£ AUGMENT: Building context...\n",
      " ‚úÖ Context prepared with retrieved document\n",
      "n3Ô∏è‚É£ GENERATE: Creating answer...\n",
      "\n",
      "==================================================\n",
      "üí¨ANSWER:\n",
      "I don't have that information in the provided documents.\n",
      "\n",
      "üìé Sources: dress_code.md, remote_work.md\n",
      "\n",
      "‚ùî Question: How many vacation days do I get?\n",
      "--------------------------------------------------\n",
      "1Ô∏è‚É£ RETRIEVE: Converting to embeddings...\n",
      "     Retrieved 3 relevant chunks\n",
      "    - remote_work.md (policies)\n",
      "    - remote_work.md (policies)\n",
      "    - dress_code.md (policies)\n",
      "\n",
      "2Ô∏è‚É£ AUGMENT: Building context...\n",
      " ‚úÖ Context prepared with retrieved document\n",
      "n3Ô∏è‚É£ GENERATE: Creating answer...\n",
      "\n",
      "==================================================\n",
      "üí¨ANSWER:\n",
      "I don't have that information in the provided documents.\n",
      "\n",
      "üìé Sources: dress_code.md, remote_work.md\n",
      "\n",
      "‚ùî Question: What is the remote work policy?\n",
      "--------------------------------------------------\n",
      "1Ô∏è‚É£ RETRIEVE: Converting to embeddings...\n",
      "     Retrieved 3 relevant chunks\n",
      "    - remote_work.md (policies)\n",
      "    - remote_work.md (policies)\n",
      "    - dress_code.md (policies)\n",
      "\n",
      "2Ô∏è‚É£ AUGMENT: Building context...\n",
      " ‚úÖ Context prepared with retrieved document\n",
      "n3Ô∏è‚É£ GENERATE: Creating answer...\n",
      "\n",
      "==================================================\n",
      "üí¨ANSWER:\n",
      "The remote work policy allows employees to work remotely up to 3 days per week with manager approval. All remote employees must remain available during core business hours (9am‚Äì3pm). Additionally, VPN usage is required when accessing internal systems.\n",
      "\n",
      "üìé Sources: dress_code.md, remote_work.md\n"
     ]
    }
   ],
   "source": [
    "def test_rag_pipeline():\n",
    "    \n",
    "    test_questions = [\n",
    "        \"Can I bring my dog to the office?\",\n",
    "        \"How many vacation days do I get?\",\n",
    "        \"What is the remote work policy?\"\n",
    "    ]\n",
    "\n",
    "    for question in test_questions:\n",
    "        answer = rag_pipeline(question)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üí¨ANSWER:\")\n",
    "        print(answer)\n",
    "\n",
    "success_1 = test_rag_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
